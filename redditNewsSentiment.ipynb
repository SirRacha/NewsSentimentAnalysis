{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import twython\n",
    "import time\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# Set your header according to the form below\n",
    "# :: (by /u/)\n",
    "\n",
    "# Add your username below\n",
    "hdr = {'User-Agent': 'windows:r/politics.single.result:v1.0' +\n",
    "       '(by /u/)'}\n",
    "url = 'https://www.reddit.com/r/politics/.json'\n",
    "req = requests.get(url, headers=hdr)\n",
    "json_data = json.loads(req.text)\n",
    "\n",
    "posts = json.dumps(json_data['data']['children'], indent=4, sort_keys=True)\n",
    "print(posts)\n",
    "\n",
    "#loop to get 1000 posts. Must have 2 sec break to not violate API rules\n",
    "\n",
    "data_all = json_data['data']['children']\n",
    "num_of_posts = 0\n",
    "while len(data_all) <= 100:\n",
    "    time.sleep(2)\n",
    "    last = data_all[-1]['data']['name']\n",
    "    url = 'https://www.reddit.com/r/politics/.json?after=' + str(last)\n",
    "    req = requests.get(url, headers=hdr)\n",
    "    data = json.loads(req.text)\n",
    "    data_all += data['data']['children']\n",
    "    if num_of_posts == len(data_all):\n",
    "        break\n",
    "    else:\n",
    "        num_of_posts = len(data_all)\n",
    "\n",
    "#Label data for NLTK\n",
    "sia = SIA()\n",
    "pos_list = []\n",
    "neg_list = []\n",
    "for post in data_all:\n",
    "    res = sia.polarity_scores(post['data']['title'])\n",
    "\n",
    "    print(res)\n",
    "    \n",
    "    if res['compound'] > 0.2:\n",
    "        pos_list.append(post['data']['title'])\n",
    "    elif res['compound'] < -0.2:\n",
    "        neg_list.append(post['data']['title'])\n",
    "\n",
    "with open(\"pos_news_titles.txt\", \"w\", encoding='utf-8',\n",
    "          errors='ignore') as f_pos:\n",
    "    for post in pos_list:\n",
    "        f_pos.write(post + \"\\n\")\n",
    "\n",
    "with open(\"neg_news_titles.txt\", \"w\", encoding='utf-8',\n",
    "          errors='ignore') as f_neg:\n",
    "    for post in neg_list:\n",
    "        f_neg.write(post + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Setup Tokenizers and Stopwords\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "example = \"This is an example sentence! However, it \" \\\n",
    "          \"is a very informative one,\"\n",
    "\n",
    "print(word_tokenize(example, language='english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print(tokenizer.tokenize(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to print ALL stop words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather and store all the positive words (meaning the words on positive headlines), and try to extract any valuable insight on them.\n",
    "all_words_pos = []\n",
    "with open(\"pos_news_titles.txt\", \"r\", encoding='utf-8',\n",
    "         errors='ignore') as f_pos:\n",
    "    for line in f_pos.readlines():\n",
    "        words = tokenizer.tokenize(line)\n",
    "        for w in words:\n",
    "            if w.lower() not in stop_words:\n",
    "                all_words_pos.append(w.lower())\n",
    "\n",
    "#frequency of each word\n",
    "pos_res = nltk.FreqDist(all_words_pos)\n",
    "print(pos_res.most_common(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather for negative words\n",
    "all_words_neg = []\n",
    "with open(\"neg_news_titles.txt\", \"r\", encoding='utf-8',\n",
    "         errors='ignore') as f_neg:\n",
    "    for line in f_neg.readlines():\n",
    "        words = tokenizer.tokenize(line)\n",
    "        for w in words:\n",
    "            if w.lower() not in stop_words:\n",
    "                all_words_neg.append(w.lower())\n",
    "\n",
    "neg_res = nltk.FreqDist(all_words_neg)\n",
    "print(neg_res.most_common(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot positive results\n",
    "#%matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Code for log-log plots\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "y_val = [x[1] for x in pos_res.most_common(len(all_words_pos))]\n",
    "y_final = []\n",
    "for i, k, z, t in zip(y_val[0::4], y_val[1::4], y_val[2::4], y_val[3::4]):\n",
    "    y_final.append(math.log(i+k+z+t))\n",
    "x_val = [math.log(i+1) for i in range(len(y_final))]\n",
    "\n",
    "plt.xlabel(\"Words (Log)\")\n",
    "plt.ylabel(\"Frequency (Log)\")\n",
    "plt.title(\"Word Frequency Distribution (Positive)\")\n",
    "plt.plot(x_val, y_final)\n",
    "plt.show()\n",
    "\n",
    "y_val = [x[1] for x in neg_res.most_common(len(all_words_neg))]\n",
    "y_final = []\n",
    "for i, k, z in zip(y_val[0::3], y_val[1::3], y_val[2::3]):\n",
    "    if i+k+z == 0:\n",
    "        break\n",
    "    y_final.append(math.log(i+k+z))\n",
    "x_val = [math.log(i+1) for i in range(len(y_final))]\n",
    "\n",
    "plt.xlabel(\"Words (Log)\")\n",
    "plt.ylabel(\"Frequency (Log)\")\n",
    "plt.title(\"Word Frequency Distribution (Negative)\")\n",
    "plt.plot(x_val, y_final)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bar chart\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# For the bar-chart distribution\n",
    "y_val = [448/982*100, 307/982*100, 227/982*100]\n",
    "x_val = [1, 2, 3]\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "ind = np.arange(len(x_val))\n",
    "width = 0.3\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(ind+0.1, y_val,width, color='green')\n",
    "ax.set_xticks(ind+0.1+width/2)\n",
    "ax.set_xticklabels(['Neutral', 'Negative', 'Positive'])\n",
    "ax.legend()\n",
    "plt.title(\"Categories Distribution\")\n",
    "plt.xlabel(\"Categories\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
